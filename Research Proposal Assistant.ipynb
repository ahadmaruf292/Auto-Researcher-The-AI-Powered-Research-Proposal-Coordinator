{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6d99685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini API key setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY=os.environ[\"GOOGLE_API_KEY\"] = \"your api key\"\n",
    "\n",
    "if GOOGLE_API_KEY:\n",
    "    print(\"Gemini API key setup complete.\")\n",
    "else:\n",
    "    print(\"Error: GOOGLE_API_KEY not found in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe3185e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ADK components imported successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.adk.agents import Agent, SequentialAgent, ParallelAgent, LoopAgent\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.models.google_llm import Gemini\n",
    "from google.adk.runners import InMemoryRunner\n",
    "from google.adk.tools import AgentTool, FunctionTool, google_search\n",
    "from google.genai import types\n",
    "\n",
    "print(\"✅ ADK components imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c49b5078",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_config=types.HttpRetryOptions(\n",
    "    attempts=10,  \n",
    "    exp_base=7,  \n",
    "    initial_delay=1,\n",
    "    http_status_codes=[429, 500, 503, 504], \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3ddcaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ research_agent created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "research_agent = Agent(\n",
    "    name=\"ResearchAgent\",\n",
    "    model=Gemini(\n",
    "        model=\"gemini-2.5-pro\",\n",
    "        retry_options=retry_config\n",
    "    ),\n",
    "    instruction=\"\"\"\n",
    "    You are a 'Researcher Agent'. Your only purpose is to find information.\n",
    "    You must use the GoogleSearchTool available to you.\n",
    "    \"\"\",\n",
    "    tools=[google_search],\n",
    "    output_key=\"raw_data\",\n",
    ")\n",
    "\n",
    "print(\"✅ research_agent created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8f3f25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ research_agent created.\n"
     ]
    }
   ],
   "source": [
    "reviewer_agent = Agent(\n",
    "    name=\"reviewer_agent\",\n",
    "    model=Gemini(\n",
    "        model=\"gemini-2.5-pro\",\n",
    "        retry_options=retry_config\n",
    "    ),\n",
    "    instruction=\"\"\"\n",
    "    You are the 'Reviewer_Agent'. You are an expert academic writer.\n",
    "    Your job is to write a 'Literature Review' section for a research proposal .\n",
    "    Read this data properly: {raw_data}\n",
    "    Write the literature review with the title of Literature review.\n",
    "    \"\"\",\n",
    "    output_key=\"literature_review\",\n",
    ")\n",
    "\n",
    "print(\"✅ research_agent created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e0161d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ research_agent created.\n"
     ]
    }
   ],
   "source": [
    "analyst_agent = Agent(\n",
    "    name=\"analyst_agent\",\n",
    "    model=Gemini(\n",
    "        model=\"gemini-2.5-pro\",\n",
    "        retry_options=retry_config\n",
    "    ),\n",
    "    instruction=\"\"\"\n",
    "    You are the 'Analyst_Agent'. You are a critical and insightful researcher.\n",
    "    Your job is to find the \"Research Gap\".\n",
    "    Read this properly: {literature_review} and {raw_data}\n",
    "    analyze literature_review,raw_data and find the research gap.\n",
    "    \"\"\",\n",
    "    output_key=\"research_gap\",\n",
    ")\n",
    "\n",
    "print(\"✅ research_agent created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe577b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ research_agent created.\n"
     ]
    }
   ],
   "source": [
    "methodology_agent = Agent(\n",
    "    name=\"methodology_agent\",\n",
    "    model=Gemini(\n",
    "        model=\"gemini-2.5-pro\",\n",
    "        retry_options=retry_config\n",
    "    ),\n",
    "    instruction=\"\"\"\n",
    "    You are the 'Methodology_Agent'. You are a practical and experienced scientist.\n",
    "    Your job is to propose a research methodology.\n",
    "    Read this properly: {research_gap}\n",
    "    analyze research_gap and propose a research methodology.\n",
    "    \"\"\",\n",
    "    output_key=\"proposed_methodology\",\n",
    ")\n",
    "\n",
    "print(\"✅ research_agent created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "968cae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ aggregator_agent created.\n"
     ]
    }
   ],
   "source": [
    "aggregator_agent = Agent(\n",
    "    name=\"AggregatorAgent\",\n",
    "    model=Gemini(\n",
    "        model=\"gemini-2.5-pro\",\n",
    "        retry_options=retry_config\n",
    "    ),\n",
    "\n",
    "    instruction=\"\"\"\n",
    "    You are the 'Aggregator_Agent'. You are an expert academic writer.\n",
    "    Your job is to compile a research proposal.\n",
    "    \n",
    "    research proposal should include the following sections:\n",
    "    section 1 \n",
    "    Literature Review:\n",
    "    {literature_review}\n",
    "    section 2\n",
    "    Research Gap:\n",
    "    {research_gap}\n",
    "    section 3\n",
    "    Methodology:\n",
    "    {proposed_methodology}\n",
    "    \n",
    "    Write a cohesive research proposal that seamlessly integrates the literature review, research gap, and proposed methodology.\"\"\",\n",
    "    tools=[AgentTool(research_agent),AgentTool(reviewer_agent),AgentTool(analyst_agent),AgentTool(methodology_agent)],\n",
    "    output_key=\"Final_Result\",\n",
    ")\n",
    "\n",
    "print(\"✅ aggregator_agent created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eff3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Coordinator_agent.\n",
      "✅ Stateful agent initialized!\n",
      "   - Application: Research Proposal Assistant\n",
      "   - User: 01\n",
      "   - Using: InMemorySessionService\n"
     ]
    }
   ],
   "source": [
    "\n",
    "APP_NAME = \"Research Proposal Assistant\"  \n",
    "USER_ID = \"01\"  \n",
    "SESSION = \"sid001\"  \n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-pro\"\n",
    "root_agent = SequentialAgent(\n",
    "    name=\"ResearchCoordinator\",\n",
    "    sub_agents=[research_agent, reviewer_agent, analyst_agent,methodology_agent, aggregator_agent],\n",
    ")\n",
    "print(\"✅ Coordinator_agent.\")\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "\n",
    "runner = Runner(agent=root_agent, app_name=APP_NAME, session_service=session_service)\n",
    "\n",
    "print(\"✅ Stateful agent initialized!\")\n",
    "print(f\"   - Application: {APP_NAME}\")\n",
    "print(f\"   - User: {USER_ID}\")\n",
    "print(f\"   - Using: {session_service.__class__.__name__}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67582935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "async def run_session(\n",
    "    runner_instance: Runner,\n",
    "    user_queries: list[str] | str = None,\n",
    "    session_name: str = \"default\",\n",
    "):\n",
    "    print(f\"\\n ### Session: {session_name}\")\n",
    "\n",
    "    \n",
    "    app_name = runner_instance.app_name\n",
    "\n",
    "    \n",
    "    try:\n",
    "        session = await session_service.create_session(\n",
    "            app_name=app_name, user_id=USER_ID, session_id=session_name\n",
    "        )\n",
    "    except:\n",
    "        session = await session_service.get_session(\n",
    "            app_name=app_name, user_id=USER_ID, session_id=session_name\n",
    "        )\n",
    "\n",
    "    \n",
    "    if user_queries:\n",
    "        \n",
    "        if type(user_queries) == str:\n",
    "            user_queries = [user_queries]\n",
    "\n",
    "        \n",
    "        for query in user_queries:\n",
    "            print(f\"\\nUser > {query}\")\n",
    "\n",
    "            \n",
    "            query = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
    "\n",
    "            \n",
    "            async for event in runner_instance.run_async(\n",
    "                user_id=USER_ID, session_id=session.id, new_message=query\n",
    "            ):\n",
    "                \n",
    "                if event.content and event.content.parts:\n",
    "                    \n",
    "                    if (\n",
    "                        event.content.parts[0].text != \"None\"\n",
    "                        and event.content.parts[0].text\n",
    "                    ):\n",
    "                        print(f\"AI > \", event.content.parts[0].text)\n",
    "    else:\n",
    "        print(\"No queries!\")\n",
    "\n",
    "\n",
    "print(\"✅ Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ca48da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ### Session: sid001\n",
      "\n",
      "User > Feature Extraction of Rice Varieties for Morphological and Visual Similarity Analysis using Deep Learning?\n",
      "AI >  ### Deep Learning Revolutionizes Rice Variety Analysis Through Automated Feature Extraction\n",
      "\n",
      "Deep learning models, particularly Convolutional Neural Networks (CNNs), are at the forefront of a technological revolution in agriculture, specifically in the morphological and visual similarity analysis of rice varieties. These advanced neural networks automatically extract key features from images of rice grains, enabling highly accurate classification and analysis that surpasses traditional, manual methods in both speed and objectivity.\n",
      "\n",
      "The core strength of deep learning in this field lies in its ability to automatically learn and identify discriminating features directly from image data. This eliminates the need for manual feature extraction, a process that is not only time-consuming but also prone to human error and subjectivity. CNNs can identify a wide range of morphological and visual characteristics that are crucial for distinguishing between different rice varieties.\n",
      "\n",
      "**Key Morphological and Visual Features Extracted by Deep Learning:**\n",
      "\n",
      "*   **Grain Shape and Size:** Deep learning models excel at capturing subtle variations in the length, width, and overall shape of rice grains. These are fundamental characteristics for variety identification.\n",
      "*   **Texture:** The surface texture of the rice grain, whether it is smooth, rough, or has other distinct patterns, is another important feature that CNNs can effectively analyze.\n",
      "*   **Color:** The color and tone of the rice grains are critical identifiers. Deep learning models can process color information in images to differentiate between varieties that may have similar shapes but different hues.\n",
      "*   **Eccentricity and Axis Length:** More complex morphological features, such as the grain's eccentricity (a measure of how much it deviates from being circular) and the lengths of its major and minor axes, are also effectively extracted.\n",
      "\n",
      "**Prominent Deep Learning Architectures and Techniques:**\n",
      "\n",
      "Several deep learning architectures have proven to be highly effective in the analysis of rice varieties. Researchers have successfully employed models such as VGG-16, VGG-19, ResNet50, DenseNet, and InceptionV3 for classification tasks. These models are often pre-trained on large image datasets and then fine-tuned on specific rice grain datasets, a technique known as **transfer learning**. This approach significantly improves classification accuracy, even with smaller datasets.\n",
      "\n",
      "For instance, a study utilizing a custom CNN model on a dataset of five rice varieties achieved a remarkable 100% classification accuracy. Another research effort employed a hybrid model combining a CNN with a Long Short-Term Memory (LSTM) network to capture both spatial features from images and sequential dependencies, achieving an accuracy of 97.8%. Furthermore, the DENS-INCEP model, which integrates DenseNet-201 and the Inception module, has demonstrated the ability to capture multi-scale shape-related features, leading to high classification accuracy.\n",
      "\n",
      "**Enhancing Transparency with Explainable AI:**\n",
      "\n",
      "To build trust and understand the decision-making process of these complex models, researchers are increasingly integrating explainable AI (XAI) techniques. Methods like LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) provide insights into which specific features of the rice grains most influence the model's classification.\n",
      "\n",
      "**Public Datasets Fueling Research:**\n",
      "\n",
      "The availability of public datasets has been instrumental in advancing research in this area. A widely used resource is the Kaggle Rice Image Dataset, which contains 75,000 images of five different rice varieties. This and similar datasets provide a valuable foundation for training and evaluating new deep learning models.\n",
      "\n",
      "The application of deep learning for feature extraction is a significant step forward in the morphological and visual similarity analysis of rice varieties. This technology offers a powerful and efficient tool for quality control, yield optimization, and trade in the agricultural sector.\n",
      "AI >  ### **Literature Review**\n",
      "\n",
      "The classification and analysis of rice varieties are critical for agricultural quality control, trade regulation, and genetic improvement. Traditionally, these tasks have relied on manual inspection by trained experts, a process that is inherently subjective, labor-intensive, and susceptible to human error. In recent years, the field has witnessed a paradigm shift towards automated, image-based analysis, with deep learning emerging as a transformative technology for extracting morphological and visual features from rice grains.\n",
      "\n",
      "The primary advantage of deep learning, particularly Convolutional Neural Networks (CNNs), lies in its capacity for automated feature extraction. Unlike conventional machine learning approaches that require manual engineering of features, CNNs can autonomously learn hierarchical and discriminative representations directly from raw pixel data. This automated process not only enhances objectivity and speed but also allows for the identification of subtle, complex patterns that may be imperceptible to the human eye. The literature confirms that CNNs adeptly capture a comprehensive range of key morphological and visual characteristics. These include fundamental attributes such as grain shape, size (length and width), and color, as well as more intricate features like surface texture, eccentricity, and the lengths of the major and minor axes, all of which are vital for accurate variety differentiation.\n",
      "\n",
      "A growing body of research demonstrates the successful application of various deep learning architectures to this problem. Prominent models such as VGG-16, VGG-19, ResNet50, DenseNet, and InceptionV3 have been widely employed for rice classification tasks. A common and highly effective strategy is the use of transfer learning, where models pre-trained on large-scale image datasets (e.g., ImageNet) are fine-tuned on specific datasets of rice grains. This approach leverages the pre-existing knowledge of the models, often leading to superior performance and reduced training time, even with limited data. The efficacy of these methods is reflected in the high accuracies reported in recent studies. For example, research utilizing a custom-built CNN achieved a perfect 100% classification accuracy on a dataset of five rice varieties. Similarly, innovative hybrid models, such as one combining a CNN with a Long Short-Term Memory (LSTM) network to process spatial and sequential information, have attained accuracies as high as 97.8%. Furthermore, sophisticated architectures like the DENS-INCEP model, which integrates DenseNet and Inception modules, have shown exceptional capability in capturing multi-scale features, contributing to highly accurate classification results.\n",
      "\n",
      "As deep learning models become more complex, their \"black-box\" nature presents a challenge to trust and interpretability. To address this, the field is increasingly incorporating Explainable AI (XAI) techniques. Methods such as LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) are being used to provide crucial insights into the decision-making process of the models. These techniques help to identify which specific visual features—such as grain tip shape or surface texture—most significantly influence a model's prediction, thereby enhancing transparency and confidence in the results.\n",
      "\n",
      "The rapid advancement in this domain has been significantly fueled by the availability of public datasets. Resources like the Kaggle Rice Image Dataset, which contains 75,000 images across five varieties, provide a standardized benchmark for training, testing, and comparing new models, fostering reproducibility and innovation within the research community. In summary, the literature unequivocally establishes that deep learning provides a powerful, efficient, and highly accurate framework for the automated feature extraction and analysis of rice varieties, marking a significant advancement over traditional methods.\n",
      "AI >  Excellent. As the Analyst_Agent, my function is to critically evaluate the provided texts to identify the research gap.\n",
      "\n",
      "After a thorough analysis of the Literature Review and the summary from the ResearchAgent, I have identified several critical research gaps.\n",
      "\n",
      "### **Analysis Summary**\n",
      "\n",
      "The provided literature establishes a strong foundation for using deep learning, particularly CNNs, for rice variety classification. It highlights high accuracy rates on benchmark datasets, the effectiveness of transfer learning, and the initial application of Explainable AI (XAI) for model transparency. The research successfully demonstrates that deep learning can automate feature extraction for tasks like shape, size, color, and texture analysis, significantly outperforming traditional methods.\n",
      "\n",
      "However, the literature primarily focuses on a well-defined, and arguably solved, problem: **high-accuracy classification on clean, limited-variety datasets.** This focus reveals several underexplored areas.\n",
      "\n",
      "---\n",
      "\n",
      "### **Identified Research Gaps**\n",
      "\n",
      "The current body of work, as presented, leaves the following significant questions and areas unaddressed:\n",
      "\n",
      "**1. Limited Scalability and Generalization to Diverse Varieties:**\n",
      "*   **The Gap:** The literature consistently refers to successes on datasets with a very small number of varieties, specifically citing the Kaggle dataset with only five. While achieving 100% accuracy on five classes is impressive, it is not representative of the real world, where hundreds or even thousands of rice varieties exist.\n",
      "*   **Implication:** There is no evidence presented to suggest how these models would scale when faced with dozens or hundreds of morphologically similar varieties. The current approach may not generalize well, and its true discriminative power in a large-scale, real-world scenario remains unproven. Future research should focus on training and validating models on much larger and more diverse varietal datasets.\n",
      "\n",
      "**2. Robustness in Real-World, Non-Ideal Conditions:**\n",
      "*   **The Gap:** The research is predicated on high-quality, standardized image datasets (e.g., single grains on a uniform background). The literature does not address the performance of these models under realistic agricultural and industrial conditions, which include:\n",
      "    *   **Impurities:** Presence of foreign materials like small stones, chaff, or seeds from other plants.\n",
      "    *   **Damaged Grains:** Broken, cracked, or discolored grains due to disease, pests, or improper handling.\n",
      "    *   **Occlusion and Bulk Imaging:** Analyzing images of overlapping grains in a pile, rather than perfectly segmented individual grains.\n",
      "*   **Implication:** A significant gap exists between the \"lab environment\" performance and practical application. Research is needed to develop models that are robust to noise, impurities, and imperfect imaging conditions, which are inevitable in quality control pipelines.\n",
      "\n",
      "**3. Overemphasis on Classification Over Quantitative Similarity Analysis:**\n",
      "*   **The Gap:** The primary goal and metric of success in the cited literature is *classification accuracy* (i.e., assigning a grain to a discrete category). The initial prompt mentions \"similarity analysis,\" but the described methods do not explicitly address the quantitative measurement of similarity. For example, they do not answer questions like: \"How similar is Variety X to Variety Y?\" or \"Find the top three most morphologically similar varieties to this new, unknown sample.\"\n",
      "*   **Implication:** The research has not yet explored the use of deep learning for creating a continuous \"morphological space.\" Future work could use techniques like metric learning or siamese networks to learn embedding vectors where the distance between vectors corresponds to the visual similarity of the rice grains. This would enable more nuanced applications like variety retrieval, anomaly detection, and genetic lineage mapping based on morphology.\n",
      "\n",
      "**4. Superficial Application of Explainable AI (XAI):**\n",
      "*   **The Gap:** The literature mentions using XAI techniques like LIME and SHAP to identify which features influence a single prediction. While this enhances transparency, it is a limited application. The research does not leverage XAI to extract global, scientifically meaningful insights.\n",
      "*   **Implication:** There is a missed opportunity to use XAI to move beyond simple model validation and towards knowledge discovery. A crucial next step is to use aggregated XAI outputs to generate a \"visual signature\" or a definitive, quantifiable morphological profile for each rice variety as understood by the model. This could validate or even challenge traditional botanical descriptions.\n",
      "\n",
      "**In summary, the primary research gap is the transition from high-performance classification in a controlled environment to robust, scalable, and nuanced analysis in a real-world context.** The current research proves the *potential* of the technology, but future work must address the challenges of scale, data imperfection, and moving beyond simple categorization to a more profound quantitative understanding of morphological similarity.\n",
      "AI >  Excellent. Based on the critical analysis of the research gaps, I will now propose a comprehensive, multi-phase research methodology designed to address these limitations and advance the field from controlled classification to robust, real-world morphological analysis.\n",
      "\n",
      "---\n",
      "\n",
      "### **Proposed Research Methodology**\n",
      "\n",
      "**Research Title:** A Framework for Robust, Scalable, and Quantitative Morphological Similarity Analysis of Rice Varieties using Deep Metric Learning and Explainable AI.\n",
      "\n",
      "**Guiding Principles:** This methodology is founded on a paradigm shift from simple classification to a more nuanced, quantitative understanding of similarity. It prioritizes real-world applicability, scalability, and the generation of scientifically valuable insights.\n",
      "\n",
      "**Overall Objective:** To develop and validate a deep learning system capable of:\n",
      "1.  Operating effectively on a large, diverse set of rice varieties.\n",
      "2.  Maintaining high performance under non-ideal, real-world imaging conditions.\n",
      "3.  Quantifying morphological similarity between varieties rather than just classifying them.\n",
      "4.  Generating aggregated, interpretable \"visual signatures\" for each variety to facilitate knowledge discovery.\n",
      "\n",
      "---\n",
      "\n",
      "### **Phase 1: Curation of a Real-World, Large-Scale Rice Dataset**\n",
      "\n",
      "This phase directly addresses Gap 1 (Scalability) and Gap 2 (Robustness).\n",
      "\n",
      "1.  **Data Sourcing and Expansion:**\n",
      "    *   **Objective:** To create a dataset that is an order of magnitude larger and more diverse than existing public benchmarks.\n",
      "    *   **Action:** Establish collaborations with agricultural research institutes, national seed banks, and commercial rice producers to source a wide range of rice varieties (target: 50-100+ distinct varieties). This will include genetically close but morphologically distinct varieties to truly test the model's discriminative power.\n",
      "\n",
      "2.  **Dual-Condition Imaging Protocol:**\n",
      "    *   **Objective:** To capture data that reflects both laboratory and industrial conditions.\n",
      "    *   **Action:** For each variety, two sets of images will be captured:\n",
      "        *   **Set A (Controlled):** Images of individual, clean grains on a uniform, high-contrast background, mimicking existing datasets. This will serve as a baseline for performance.\n",
      "        *   **Set B (Realistic):** Images of rice grains under simulated real-world conditions. This will include:\n",
      "            *   **Bulk Imaging:** Piles of grains with partial occlusion.\n",
      "            *   **Impurities:** Introduction of common foreign materials (e.g., small pebbles, chaff, broken grains).\n",
      "            *   **Damage Simulation:** Inclusion of samples with known defects (e.g., discoloration, cracks).\n",
      "\n",
      "3.  **Preprocessing and Annotation:**\n",
      "    *   **Objective:** To prepare the data for robust model training.\n",
      "    *   **Action:**\n",
      "        *   All images will be annotated with the variety label. For Set B, bounding boxes will be used to identify individual grains, impurities, and damaged grains.\n",
      "        *   An initial object detection model (e.g., YOLOv5 or Faster R-CNN) will be trained to segment and extract individual grains from the complex 'Realistic' images (Set B). This isolates grains before they are passed to the core similarity model.\n",
      "        *   Standard image augmentation (rotation, flipping, brightness/contrast adjustment) will be applied to both sets to increase model generalization.\n",
      "\n",
      "---\n",
      "\n",
      "### **Phase 2: Development of a Deep Metric Learning Model**\n",
      "\n",
      "This phase directly addresses Gap 3 (Quantitative Similarity Analysis).\n",
      "\n",
      "1.  **Paradigm Shift from Classification to Embedding:**\n",
      "    *   **Objective:** To learn a feature space where the distance between two points directly corresponds to the morphological similarity of the rice grains.\n",
      "    *   **Action:** We will move away from a standard classification architecture with a softmax output. Instead, we will implement a **Deep Metric Learning (DML)** framework.\n",
      "\n",
      "2.  **Model Architecture:**\n",
      "    *   **Backbone:** A state-of-the-art pre-trained CNN (e.g., EfficientNetB4, ResNet50) will be used as the feature extractor backbone. This leverages transfer learning.\n",
      "    *   **Head:** The final classification layer of the CNN will be replaced with a dense layer that outputs a fixed-size embedding vector (e.g., 128 or 256 dimensions).\n",
      "    *   **Learning Framework:** A **Triplet Network** architecture will be employed.\n",
      "        *   The model is fed three images at a time: an **Anchor** (a rice grain of a certain variety), a **Positive** (another grain of the *same* variety), and a **Negative** (a grain of a *different* variety).\n",
      "        *   The model will be trained using a **Triplet Loss function**. This loss function's objective is to minimize the distance between the Anchor and the Positive embeddings while simultaneously maximizing the distance between the Anchor and the Negative embeddings in the N-dimensional space.\n",
      "\n",
      "---\n",
      "\n",
      "### **Phase 3: Comprehensive Experimental Validation**\n",
      "\n",
      "This phase validates the system against the identified gaps.\n",
      "\n",
      "1.  **Baseline Performance and Scalability Test (Addresses Gap 1):**\n",
      "    *   **Procedure:** The trained model's embedding space will be used for a large-scale classification task (using a k-Nearest Neighbors algorithm on the embeddings). This will be performed on the 'Controlled' dataset (Set A) with all 50-100+ varieties.\n",
      "    *   **Metric:** Top-1 and Top-5 accuracy. This will demonstrate if the model can effectively discriminate between a large number of varieties.\n",
      "\n",
      "2.  **Robustness and Real-World Performance Test (Addresses Gap 2):**\n",
      "    *   **Procedure:** The model, trained on both Set A and the segmented grains from Set B, will be evaluated on unseen 'Realistic' images. This tests the entire pipeline, from grain segmentation to similarity analysis.\n",
      "    *   **Metric:** Mean Average Precision (mAP) for retrieval tasks and classification accuracy under noisy conditions. Performance will be compared to a model trained only on clean data.\n",
      "\n",
      "3.  **Quantitative Similarity and Retrieval Task (Addresses Gap 3):**\n",
      "    *   **Procedure:** A user interface will be developed where an image of a new rice grain can be provided as a query. The system will use the embedding model to retrieve the top 5 most morphologically similar varieties from the database by finding the nearest neighbors in the embedding space.\n",
      "    *   **Metric:** The results will be evaluated qualitatively by domain experts (agronomists/botanists) and quantitatively using rank-based metrics.\n",
      "\n",
      "---\n",
      "\n",
      "### **Phase 4: Knowledge Discovery via Aggregated XAI**\n",
      "\n",
      "This phase directly addresses Gap 4 (Superficial XAI Application).\n",
      "\n",
      "1.  **Objective:** To move beyond single-instance explanations and generate a global, interpretable \"visual signature\" for each rice variety as understood by the model.\n",
      "\n",
      "2.  **Methodology:**\n",
      "    *   **Technique Selection:** A gradient-based XAI method such as **Grad-CAM++** or **Integrated Gradients** will be used, as these methods produce high-resolution saliency maps highlighting the pixels most influential to the embedding.\n",
      "    *   **Aggregation Procedure:**\n",
      "        1.  For each variety, process a large number (e.g., 1000) of its images through the trained embedding model.\n",
      "        2.  For each image, generate a saliency map.\n",
      "        3.  Normalize and align all saliency maps for that variety.\n",
      "        4.  Compute an **Average Saliency Map** by pixel-wise averaging. This composite map represents the model's generalized attention for that variety, filtering out instance-specific noise.\n",
      "\n",
      "3.  **Validation and Interpretation:**\n",
      "    *   **Action:** These generated \"visual signatures\" (the average saliency maps) will be presented to agricultural experts.\n",
      "    *   **Goal:** To determine if the features highlighted by the model (e.g., the tip, the germ, the surface texture) align with, or perhaps even challenge and augment, traditional botanical descriptors used for variety identification. This transforms the XAI from a transparency tool into a knowledge discovery engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI >  ### **Research Proposal**\n",
      "\n",
      "**Research Title:** A Framework for Robust, Scalable, and Quantitative Morphological Similarity Analysis of Rice Varieties using Deep Metric Learning and Explainable AI.\n",
      "\n",
      "### **1. Literature Review**\n",
      "\n",
      "The classification and analysis of rice varieties are critical for agricultural quality control, trade regulation, and genetic improvement. Traditionally, these tasks have relied on manual inspection by trained experts, a process that is inherently subjective, labor-intensive, and susceptible to human error. In recent years, the field has witnessed a paradigm shift towards automated, image-based analysis, with deep learning emerging as a transformative technology for extracting morphological and visual features from rice grains.\n",
      "\n",
      "The primary advantage of deep learning, particularly Convolutional Neural Networks (CNNs), lies in its capacity for automated feature extraction. Unlike conventional machine learning approaches that require manual engineering of features, CNNs can autonomously learn hierarchical and discriminative representations directly from raw pixel data. This automated process not only enhances objectivity and speed but also allows for the identification of subtle, complex patterns that may be imperceptible to the human eye. The literature confirms that CNNs adeptly capture a comprehensive range of key morphological and visual characteristics. These include fundamental attributes such as grain shape, size (length and width), and color, as well as more intricate features like surface texture, eccentricity, and the lengths of the major and minor axes, all of which are vital for accurate variety differentiation.\n",
      "\n",
      "A growing body of research demonstrates the successful application of various deep learning architectures to this problem. Prominent models such as VGG-16, VGG-19, ResNet50, DenseNet, and InceptionV3 have been widely employed for rice classification tasks. A common and highly effective strategy is the use of transfer learning, where models pre-trained on large-scale image datasets (e.g., ImageNet) are fine-tuned on specific datasets of rice grains. This approach leverages the pre-existing knowledge of the models, often leading to superior performance and reduced training time, even with limited data. The efficacy of these methods is reflected in the high accuracies reported in recent studies. For example, research utilizing a custom-built CNN achieved a perfect 100% classification accuracy on a dataset of five rice varieties. Similarly, innovative hybrid models, such as one combining a CNN with a Long Short-Term Memory (LSTM) network to process spatial and sequential information, have attained accuracies as high as 97.8%.\n",
      "\n",
      "As deep learning models become more complex, their \"black-box\" nature presents a challenge to trust and interpretability. To address this, the field is increasingly incorporating Explainable AI (XAI) techniques. Methods such as LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) are being used to provide crucial insights into the decision-making process of the models, thereby enhancing transparency.\n",
      "\n",
      "The rapid advancement in this domain has been significantly fueled by the availability of public datasets. Resources like the Kaggle Rice Image Dataset, which contains 75,000 images across five varieties, provide a standardized benchmark for training and testing new models. In summary, the literature unequivocally establishes that deep learning provides a powerful, efficient, and highly accurate framework for the automated feature extraction and analysis of rice varieties, marking a significant advancement over traditional methods.\n",
      "\n",
      "### **2. Research Gap and Problem Statement**\n",
      "\n",
      "While the literature establishes a strong foundation for using deep learning for rice variety classification, it primarily focuses on a well-defined, and arguably solved, problem: **high-accuracy classification on clean, limited-variety datasets.** This focus reveals several critical gaps between current academic success and real-world applicability. The primary research gap is the transition from high-performance classification in a controlled environment to a robust, scalable, and nuanced analysis suitable for a real-world context.\n",
      "\n",
      "The current body of work leaves the following significant areas unaddressed:\n",
      "\n",
      "1.  **Limited Scalability and Generalization:** Successes are consistently reported on datasets with a very small number of varieties (e.g., five). There is no evidence to suggest how these models would scale when faced with the hundreds of morphologically similar varieties that exist in reality.\n",
      "2.  **Robustness in Non-Ideal Conditions:** Research is predicated on high-quality, standardized images of single grains on uniform backgrounds. Performance under realistic agricultural conditions—with impurities, damaged grains, and occluded bulk images—is unaddressed.\n",
      "3.  **Overemphasis on Classification over Quantitative Similarity:** The primary metric of success is classification accuracy. The literature has not yet explored the use of deep learning to create a continuous \"morphological space\" that can quantitatively measure similarity (e.g., \"How similar is Variety X to Variety Y?\").\n",
      "4.  **Superficial Application of Explainable AI (XAI):** XAI is currently used to explain single predictions. There is a missed opportunity to aggregate XAI outputs to generate global, scientifically meaningful insights, such as a definitive \"visual signature\" for each rice variety.\n",
      "\n",
      "This research aims to address these gaps by developing a framework that moves beyond simple classification towards a comprehensive, quantitative, and interpretable analysis of rice morphology that is robust enough for practical deployment.\n",
      "\n",
      "### **3. Proposed Research Methodology**\n",
      "\n",
      "To address the identified limitations, this proposal outlines a comprehensive, multi-phase research methodology founded on a paradigm shift from simple classification to a more nuanced, quantitative understanding of similarity.\n",
      "\n",
      "**Overall Objective:** To develop and validate a deep learning system capable of:\n",
      "1.  Operating effectively on a large, diverse set of rice varieties.\n",
      "2.  Maintaining high performance under non-ideal, real-world imaging conditions.\n",
      "3.  Quantifying morphological similarity between varieties rather than just classifying them.\n",
      "4.  Generating aggregated, interpretable \"visual signatures\" for each variety to facilitate knowledge discovery.\n",
      "\n",
      "---\n",
      "\n",
      "#### **Phase 1: Curation of a Real-World, Large-Scale Rice Dataset**\n",
      "\n",
      "This phase directly addresses the gaps in scalability and robustness.\n",
      "\n",
      "1.  **Data Sourcing and Expansion:** Establish collaborations with agricultural research institutes and seed banks to source a wide range of rice varieties (target: 50-100+ distinct varieties), including genetically close and morphologically similar ones.\n",
      "2.  **Dual-Condition Imaging Protocol:** For each variety, two image sets will be captured:\n",
      "    *   **Set A (Controlled):** Images of individual, clean grains on a uniform background to serve as a baseline.\n",
      "    *   **Set B (Realistic):** Images simulating real-world conditions, including bulk imaging (piles with occlusion), impurities (chaff, pebbles), and damaged grains.\n",
      "3.  **Preprocessing and Annotation:** All images will be annotated with variety labels. For Set B, an object detection model (e.g., YOLOv5) will be trained to segment individual grains from complex scenes. Standard image augmentation will be applied to both sets to improve generalization.\n",
      "\n",
      "---\n",
      "\n",
      "#### **Phase 2: Development of a Deep Metric Learning Model**\n",
      "\n",
      "This phase addresses the gap in quantitative similarity analysis.\n",
      "\n",
      "1.  **Paradigm Shift from Classification to Embedding:** We will implement a **Deep Metric Learning (DML)** framework to learn a feature space where the distance between two points directly corresponds to the morphological similarity of the rice grains.\n",
      "2.  **Model Architecture:** A state-of-the-art pre-trained CNN (e.g., EfficientNetB4) will be used as the feature extractor backbone. Its final classification layer will be replaced with a dense layer outputting a fixed-size embedding vector (e.g., 128 dimensions).\n",
      "3.  **Learning Framework:** A **Triplet Network** architecture will be employed. The model will be trained using a **Triplet Loss function**, which minimizes the distance between embeddings of the same variety (Anchor-Positive) while maximizing the distance between embeddings of different varieties (Anchor-Negative).\n",
      "\n",
      "---\n",
      "\n",
      "#### **Phase 3: Comprehensive Experimental Validation**\n",
      "\n",
      "This phase validates the system against the identified gaps.\n",
      "\n",
      "1.  **Scalability Test:** The model's embedding space will be used for a large-scale classification task (using k-Nearest Neighbors) on the 'Controlled' dataset (Set A) with all 50-100+ varieties to measure Top-1 and Top-5 accuracy.\n",
      "2.  **Robustness Test:** The full pipeline (segmentation and embedding) will be evaluated on unseen 'Realistic' images (Set B) to measure performance under noisy conditions, using metrics like Mean Average Precision (mAP) for retrieval tasks.\n",
      "3.  **Quantitative Similarity and Retrieval Task:** A user interface will be developed to query the system with a novel rice image. The system will retrieve the top-N most morphologically similar varieties from the database by finding the nearest neighbors in the embedding space, with results evaluated by domain experts.\n",
      "\n",
      "---\n",
      "\n",
      "#### **Phase 4: Knowledge Discovery via Aggregated XAI**\n",
      "\n",
      "This phase addresses the superficial application of XAI.\n",
      "\n",
      "1.  **Objective:** To generate a global, interpretable \"visual signature\" for each rice variety as understood by the model.\n",
      "2.  **Methodology:** A gradient-based XAI method like **Grad-CAM++** will be used to produce saliency maps highlighting influential pixels for each image's embedding.\n",
      "3.  **Aggregation and Interpretation:** For each variety, hundreds of saliency maps will be generated, normalized, aligned, and averaged to create a composite **Average Saliency Map**. This map represents the model's generalized attention. These \"visual signatures\" will be presented to agricultural experts to validate if the model's focus areas align with, or augment, traditional botanical descriptors, thus transforming XAI from a transparency tool into a knowledge discovery engine.\n",
      "\n",
      "User > Do you remember the previous topic we talked about?\n",
      "AI >  Yes, I remember our previous conversation. We discussed a detailed research proposal titled: **\"A Framework for Robust, Scalable, and Quantitative Morphological Similarity Analysis of Rice Varieties using Deep Metric Learning and Explainable AI.\"**\n",
      "\n",
      "The proposal was structured to address specific gaps identified in the existing literature. It began with a **Literature Review** that established the current state of using deep learning for rice classification. This was followed by a **Research Gap** analysis, which pointed out key limitations in current research, namely:\n",
      "\n",
      "1.  **Limited Scalability:** Current models are tested on only a few rice varieties.\n",
      "2.  **Lack of Robustness:** Models are not tested under real-world conditions with impurities or damaged grains.\n",
      "3.  **Focus on Classification:** The goal is simple categorization rather than a more nuanced quantitative similarity analysis.\n",
      "4.  **Superficial Use of XAI:** Explainable AI is used for individual predictions but not for generating broader scientific insights.\n",
      "\n",
      "To address these gaps, a four-phase **Research Methodology** was proposed:\n",
      "\n",
      "*   **Phase 1:** Create a large-scale, \"real-world\" dataset with many more varieties and images that include imperfections.\n",
      "*   **Phase 2:** Develop a Deep Metric Learning model (using a Triplet Network) to learn a feature space for quantitative similarity, rather than just classification.\n",
      "*   **Phase 3:** Conduct comprehensive validation to test for scalability, robustness, and the model's ability to perform similarity-based retrieval.\n",
      "*   **Phase 4:** Use aggregated XAI methods (like Grad-CAM++) to create \"visual signatures\" for each rice variety, turning the AI into a tool for knowledge discovery.\n",
      "AI >  Of course. Based on the detailed context you've provided about the research proposal, here is the \"Literature Review\" section, written in a formal, academic style.\n",
      "\n",
      "***\n",
      "\n",
      "### **Literature Review**\n",
      "\n",
      "The classification and analysis of rice varieties are critical for agricultural quality control, trade regulation, and genetic improvement. Traditionally, these tasks have relied on manual inspection by trained experts, a process that is inherently subjective, labor-intensive, and susceptible to human error. In recent years, the field has witnessed a paradigm shift towards automated, image-based analysis, with deep learning emerging as a transformative technology for extracting morphological and visual features from rice grains.\n",
      "\n",
      "The primary advantage of deep learning, particularly Convolutional Neural Networks (CNNs), lies in its capacity for automated feature extraction. Unlike conventional machine learning approaches that require manual engineering of features, CNNs can autonomously learn hierarchical and discriminative representations directly from raw pixel data. This automated process not only enhances objectivity and speed but also allows for the identification of subtle, complex patterns that may be imperceptible to the human eye. The literature confirms that CNNs adeptly capture a comprehensive range of key morphological and visual characteristics. These include fundamental attributes such as grain shape, size (length and width), and color, as well as more intricate features like surface texture, eccentricity, and the lengths of the major and minor axes, all of which are vital for accurate variety differentiation.\n",
      "\n",
      "A growing body of research demonstrates the successful application of various deep learning architectures to this problem. Prominent models such as VGG-16, VGG-19, ResNet50, DenseNet, and InceptionV3 have been widely employed for rice classification tasks. A common and highly effective strategy is the use of transfer learning, where models pre-trained on large-scale image datasets (e.g., ImageNet) are fine-tuned on specific datasets of rice grains. This approach leverages the pre-existing knowledge of the models, often leading to superior performance and reduced training time. The efficacy of these methods is reflected in the high accuracies reported in recent studies. For example, research utilizing a custom-built CNN achieved a perfect 100% classification accuracy on a dataset of five rice varieties. Similarly, innovative hybrid models, such as one combining a CNN with a Long Short-Term Memory (LSTM) network, have attained accuracies as high as 97.8%.\n",
      "\n",
      "As deep learning models become more complex, their \"black-box\" nature presents a challenge to trust and interpretability. To address this, the field is increasingly incorporating Explainable AI (XAI) techniques. Methods such as LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) are being used to provide crucial insights into the decision-making process of the models for individual predictions, thereby enhancing transparency and confidence in the results.\n",
      "\n",
      "The rapid advancement in this domain has been significantly fueled by the availability of public datasets. Resources like the Kaggle Rice Image Dataset, which contains 75,000 images across five varieties, provide a standardized benchmark for training, testing, and comparing new models. In summary, the literature unequivocally establishes that deep learning provides a powerful, efficient, and highly accurate framework for the automated feature extraction and *classification* of rice varieties. However, the current body of work has predominantly focused on achieving high performance in controlled environments on a limited number of classes, setting the stage for critical next steps toward real-world applicability and more nuanced analysis.\n",
      "AI >  Of course. As the Analyst_Agent, my role is to critically assess the provided literature and identify the research gap. Based on a thorough analysis of all the provided context, here is the identification and explanation of the research gaps.\n",
      "\n",
      "---\n",
      "\n",
      "### **Analysis of the State of the Art**\n",
      "\n",
      "The provided literature review and research summaries collectively establish a strong, coherent baseline: deep learning, particularly using Convolutional Neural Networks (CNNs), has been successfully applied to the problem of rice variety *classification*. The existing research has proven several key points:\n",
      "\n",
      "*   **Automation:** CNNs can automatically extract complex morphological features (shape, size, texture, color), surpassing manual, subjective methods.\n",
      "*   **High Accuracy:** On controlled, clean datasets with a limited number of classes (specifically citing a 5-variety dataset), models can achieve near-perfect classification accuracy (97-100%).\n",
      "*   **Effective Techniques:** Transfer learning is a highly effective strategy for achieving these results with less data and training time.\n",
      "*   **Initial Transparency:** Explainable AI (XAI) techniques like LIME and SHAP are being used to understand model decisions on a case-by-case basis.\n",
      "\n",
      "However, this successful demonstration of capability in a narrow, controlled context is precisely what highlights the significant gaps between current academic research and the development of a truly practical, scalable, and insightful system.\n",
      "\n",
      "### **Identified Research Gaps**\n",
      "\n",
      "The current body of work, while foundational, is limited by its focus on a solved problem (high-accuracy classification in a lab setting) and fails to address the complexities of real-world application. The primary research gap is the **transition from classification in a controlled environment to robust, scalable, and nuanced quantitative analysis in a real-world context.** This overarching gap can be broken down into four critical, interconnected areas:\n",
      "\n",
      "**1. Limited Scalability and Generalization to Diverse Varieties:**\n",
      "*   **The Gap:** The literature consistently cites high performance on datasets with a very small number of varieties (e.g., five). There is no evidence to suggest how these classification models would perform when scaled to face the dozens or even hundreds of morphologically similar varieties that exist in agriculture and commerce. The discriminative power required to distinguish between 5 varieties is fundamentally different from that required to distinguish between 100.\n",
      "*   **Implication:** The real-world utility of the current models is unproven. A model that is 100% accurate on 5 classes may fail completely when faced with a more realistic and challenging number of closely related classes.\n",
      "\n",
      "**2. Lack of Robustness in Real-World, Non-Ideal Conditions:**\n",
      "*   **The Gap:** The research is predicated on high-quality, standardized images (e.g., clean, individual grains on a uniform background). The literature does not address model performance under realistic conditions, which invariably include impurities (chaff, stones), damaged grains (broken, cracked, discolored), and complex imaging scenarios (piles of overlapping grains).\n",
      "*   **Implication:** A significant gap exists between \"laboratory\" performance and practical applicability. Models trained only on pristine data are likely to be brittle and unreliable when deployed in an industrial quality control pipeline or an agricultural research setting.\n",
      "\n",
      "**3. Overemphasis on Classification Over Quantitative Similarity Analysis:**\n",
      "*   **The Gap:** The primary objective and metric of success in the cited literature is *classification accuracy*—assigning a grain to a discrete, predefined category. The research has not explored the more nuanced task of quantitatively measuring morphological similarity. It cannot answer questions like, \"How similar is Variety X to Variety Y?\" or \"Given a new sample, find its three most similar registered varieties.\"\n",
      "*   **Implication:** The current paradigm limits the system's application. A shift from classification to a continuous \"morphological feature space\" (e.g., via Deep Metric Learning) would enable more sophisticated applications like similarity-based retrieval, anomaly detection, and discovering relationships between varieties based on their visual phenotypes.\n",
      "\n",
      "**4. Superficial Application of Explainable AI (XAI):**\n",
      "*   **The Gap:** The literature describes using XAI to explain *individual predictions* (i.e., \"Why was *this specific grain* classified as Arborio?\"). While useful for transparency, this is a limited, localized application. The research does not leverage XAI to extract global, aggregated, and scientifically meaningful insights.\n",
      "*   **Implication:** There is a critical missed opportunity to transform the AI from a simple classification tool into a knowledge discovery engine. By aggregating XAI outputs across thousands of samples, one could generate a definitive \"visual signature\" for each variety, highlighting the stable, key morphological traits it has learned. This could be used to validate, refine, or even challenge traditional botanical descriptors.\n",
      "AI >  Of course. As the Methodology_Agent, I have analyzed the research gaps identified by the Analyst_Agent. The core challenge is to evolve from a limited, lab-based classification system into a robust, scalable, and insightful analytical tool for real-world applications.\n",
      "\n",
      "Based on this analysis, I propose the following comprehensive research methodology.\n",
      "\n",
      "---\n",
      "\n",
      "### **Proposed Research Methodology**\n",
      "\n",
      "**Research Title:** A Framework for Robust, Scalable, and Quantitative Morphological Similarity Analysis of Rice Varieties using Deep Metric Learning and Explainable AI.\n",
      "\n",
      "**Guiding Principles:** This methodology is founded on a paradigm shift from simple classification to a more nuanced, quantitative understanding of similarity. It prioritizes real-world applicability, scalability, and the generation of scientifically valuable insights.\n",
      "\n",
      "**Overall Objective:** To develop and validate a deep learning system capable of:\n",
      "1.  Operating effectively on a large, diverse set of rice varieties.\n",
      "2.  Maintaining high performance under non-ideal, real-world imaging conditions.\n",
      "3.  Quantifying morphological similarity between varieties rather than just classifying them.\n",
      "4.  Generating aggregated, interpretable \"visual signatures\" for each variety to facilitate knowledge discovery.\n",
      "\n",
      "---\n",
      "\n",
      "### **Phase 1: Curation of a Real-World, Large-Scale Rice Dataset**\n",
      "\n",
      "This phase directly addresses Gap 1 (Scalability) and Gap 2 (Robustness). The foundation of a robust model is robust data.\n",
      "\n",
      "1.  **Data Sourcing and Expansion:**\n",
      "    *   **Objective:** To create a dataset that is an order of magnitude larger and more diverse than existing public benchmarks.\n",
      "    *   **Action:** Establish collaborations with agricultural research institutes, national seed banks, and commercial rice producers to source a wide range of rice varieties (target: **50-100+ distinct varieties**). Critically, this will include genetically close but morphologically distinct varieties to truly test the model's discriminative power.\n",
      "\n",
      "2.  **Dual-Condition Imaging Protocol:**\n",
      "    *   **Objective:** To capture data that reflects both laboratory and industrial conditions.\n",
      "    *   **Action:** For each variety, two sets of images will be captured:\n",
      "        *   **Set A (Controlled):** Images of individual, clean grains on a uniform, high-contrast background, mimicking existing datasets. This will serve as a performance baseline.\n",
      "        *   **Set B (Realistic):** Images of rice grains under simulated real-world conditions. This will include:\n",
      "            *   **Bulk Imaging:** Piles of grains with partial occlusion.\n",
      "            *   **Impurities:** Introduction of common foreign materials (e.g., small pebbles, chaff, broken grains).\n",
      "            *   **Damage Simulation:** Inclusion of samples with known defects (e.g., discoloration, cracks).\n",
      "\n",
      "3.  **Preprocessing and Annotation:**\n",
      "    *   **Objective:** To prepare the data for robust model training.\n",
      "    *   **Action:**\n",
      "        *   All images will be annotated with the variety label. For Set B, bounding boxes will be used to identify individual grains, impurities, and damaged grains.\n",
      "        *   An initial object detection model (e.g., YOLOv5 or Faster R-CNN) will be trained to segment and extract individual grains from the complex 'Realistic' images (Set B). This isolates grains before they are passed to the core similarity model.\n",
      "        *   Standard image augmentation (rotation, flipping, brightness/contrast adjustment) will be applied to both sets to increase model generalization.\n",
      "\n",
      "---\n",
      "\n",
      "### **Phase 2: Development of a Deep Metric Learning Model**\n",
      "\n",
      "This phase directly addresses Gap 3 (Quantitative Similarity Analysis).\n",
      "\n",
      "1.  **Paradigm Shift from Classification to Embedding:**\n",
      "    *   **Objective:** To learn a feature space where the distance between two points directly corresponds to the morphological similarity of the rice grains.\n",
      "    *   **Action:** We will move away from a standard classification architecture with a softmax output. Instead, we will implement a **Deep Metric Learning (DML)** framework.\n",
      "\n",
      "2.  **Model Architecture:**\n",
      "    *   **Backbone:** A state-of-the-art pre-trained CNN (e.g., EfficientNetB4, ResNet50) will be used as the feature extractor backbone. This leverages transfer learning for powerful feature extraction.\n",
      "    *   **Head:** The final classification layer of the CNN will be replaced with a dense layer that outputs a fixed-size embedding vector (e.g., 128 or 256 dimensions).\n",
      "    *   **Learning Framework:** A **Triplet Network** architecture will be employed.\n",
      "        *   The model is fed three images at a time: an **Anchor** (a rice grain of a certain variety), a **Positive** (another grain of the *same* variety), and a **Negative** (a grain of a *different* variety).\n",
      "        *   The model will be trained using a **Triplet Loss function**. This loss function's objective is to minimize the distance between the Anchor and the Positive embeddings while simultaneously maximizing the distance between the Anchor and the Negative embeddings in the N-dimensional space.\n",
      "\n",
      "---\n",
      "\n",
      "### **Phase 3: Comprehensive Experimental Validation**\n",
      "\n",
      "This phase validates the system's performance against the identified gaps.\n",
      "\n",
      "1.  **Scalability Test (Addresses Gap 1):**\n",
      "    *   **Procedure:** The trained model's embedding space will be used for a large-scale classification task (using a k-Nearest Neighbors algorithm on the embeddings). This will be performed on the 'Controlled' dataset (Set A) with all 50-100+ varieties.\n",
      "    *   **Metric:** Top-1 and Top-5 accuracy. This will demonstrate if the model can effectively discriminate between a large number of varieties.\n",
      "\n",
      "2.  **Robustness and Real-World Performance Test (Addresses Gap 2):**\n",
      "    *   **Procedure:** The model, trained on both Set A and the segmented grains from Set B, will be evaluated on unseen 'Realistic' images. This tests the entire pipeline, from grain segmentation to similarity analysis.\n",
      "    *   **Metric:** Mean Average Precision (mAP) for retrieval tasks and classification accuracy under noisy conditions. Performance will be compared to a model trained only on clean data.\n",
      "\n",
      "3.  **Quantitative Similarity and Retrieval Task (Addresses Gap 3):**\n",
      "    *   **Procedure:** A prototype system will be developed where an image of a new rice grain can be provided as a query. The system will use the embedding model to retrieve the top 5 most morphologically similar varieties from the database by finding the nearest neighbors in the embedding space.\n",
      "    *   **Metric:** The results will be evaluated qualitatively by domain experts (agronomists/botanists) and quantitatively using rank-based metrics.\n",
      "\n",
      "---\n",
      "\n",
      "### **Phase 4: Knowledge Discovery via Aggregated XAI**\n",
      "\n",
      "This phase directly addresses Gap 4 (Superficial XAI Application).\n",
      "\n",
      "1.  **Objective:** To move beyond single-instance explanations and generate a global, interpretable \"visual signature\" for each rice variety as understood by the model.\n",
      "\n",
      "2.  **Methodology:**\n",
      "    *   **Technique Selection:** A gradient-based XAI method such as **Grad-CAM++** or **Integrated Gradients** will be used, as these methods produce high-resolution saliency maps highlighting the pixels most influential to the embedding.\n",
      "    *   **Aggregation Procedure:**\n",
      "        1.  For each variety, process a large number (e.g., 1000) of its images through the trained embedding model.\n",
      "        2.  For each image, generate a saliency map.\n",
      "        3.  Normalize and align all saliency maps for that variety (e.g., by orienting them along their major axis).\n",
      "        4.  Compute an **Average Saliency Map** by pixel-wise averaging. This composite map represents the model's generalized attention for that variety, filtering out instance-specific noise.\n",
      "\n",
      "3.  **Validation and Interpretation:**\n",
      "    *   **Action:** These generated \"visual signatures\" (the average saliency maps) will be presented to agricultural experts.\n",
      "    *   **Goal:** To determine if the features highlighted by the model (e.g., the shape of the tip, the germ, the surface texture) align with, or perhaps even challenge and augment, traditional botanical descriptors used for variety identification. This transforms the XAI from a transparency tool into a knowledge discovery engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI >  Of course. I remember our conversation in detail.\n",
      "\n",
      "We discussed and formulated a comprehensive research proposal titled: **\"A Framework for Robust, Scalable, and Quantitative Morphological Similarity Analysis of Rice Varieties using Deep Metric Learning and Explainable AI.\"**\n",
      "\n",
      "The proposal was built upon a logical progression:\n",
      "\n",
      "1.  **Literature Review:** We first established that current research successfully uses deep learning (CNNs) for classifying rice varieties with high accuracy, but only in controlled environments with a very limited number of classes (e.g., five).\n",
      "\n",
      "2.  **Identified Research Gaps:** We then critically analyzed this state-of-the-art to identify four key weaknesses:\n",
      "    *   **Lack of Scalability:** The methods are not proven to work on the hundreds of varieties that exist in the real world.\n",
      "    *   **Lack of Robustness:** The models are not tested against real-world image conditions, such as impurities, damaged grains, or piles of overlapping grains.\n",
      "    *   **Overemphasis on Classification:** The research focuses on assigning a simple category label rather than quantitatively measuring the degree of similarity between different varieties.\n",
      "    *   **Superficial Use of XAI:** Explainable AI is only used to understand single predictions, not to extract broader, scientifically valuable insights.\n",
      "\n",
      "3.  **Proposed Methodology:** Finally, to address these gaps, we designed a four-phase research plan:\n",
      "    *   **Phase 1:** To create a new, large-scale, and realistic dataset with 50-100+ varieties, including images with real-world imperfections.\n",
      "    *   **Phase 2:** To develop a **Deep Metric Learning** model (specifically a Triplet Network) that learns to map similar grains close together in a feature space, enabling quantitative similarity analysis.\n",
      "    *   **Phase 3:** To rigorously validate the model's scalability, its robustness on imperfect data, and its practical ability to retrieve similar varieties.\n",
      "    *   **Phase 4:** To use aggregated XAI techniques to generate a unique \"visual signature\" for each rice variety, turning the model into a knowledge discovery tool for agricultural science.\n"
     ]
    }
   ],
   "source": [
    "await run_session(\n",
    "    runner,\n",
    "    [\n",
    "        \"Feature Extraction of Rice Varieties for Morphological and Visual Similarity Analysis using Deep Learning?\",\n",
    "        \"Do you remember the previous topic we talked about?\",  \n",
    "    ],\n",
    "    \"sid001\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad9857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logging function ready.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "async def save_full_session_log(session_service, app_name, user_id, session_id):\n",
    "\n",
    "    \n",
    "    try:\n",
    "        newsession = await session_service.get_session(\n",
    "            app_name=app_name, user_id=user_id, session_id=session_id\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\" session not found: {e}\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    log_data = {\n",
    "        \"session_meta\": {\n",
    "            \"app_name\": app_name,\n",
    "            \"session_id\": session_id,\n",
    "            \"saved_at\": timestamp\n",
    "        },\n",
    "        \"conversation_history\": []\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    if hasattr(newsession, 'events'):\n",
    "        for msg in newsession.message:\n",
    "            \n",
    "            content_text = \"\"\n",
    "            if msg.content and msg.content.parts:\n",
    "                parts_text = []\n",
    "                for p in msg.content.parts:\n",
    "                    if hasattr(p, 'text') and p.text:\n",
    "                        parts_text.append(p.text)\n",
    "                content_text = \" \".join(parts_text)\n",
    "            \n",
    "            \n",
    "            entry = {\n",
    "                \"role\": msg.role,  \n",
    "                \"content\": content_text\n",
    "            }\n",
    "            log_data[\"conversation_history\"].append(entry)\n",
    "\n",
    "\n",
    "    filename = f\"Research_Log_{session_id}_{timestamp}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(log_data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"✅ Observability Log saved succesfully: {filename}\")\n",
    "        print(f\"📂 fille path: {os.path.abspath(filename)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"problem to save log: {e}\")\n",
    "\n",
    "print(\"✅ Logging function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd5f4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Two separate functions defined: 'run_session_chat' and 'save_log_manual'\n"
     ]
    }
   ],
   "source": [
    "async def save_log_manual(session_name: str):\n",
    "    print(f\"\\n--- 💾 Saving Logs for: {session_name} ---\")\n",
    "    \n",
    "    app_name = APP_NAME \n",
    "    user_id = USER_ID   \n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        fresh_session = await session_service.get_session(\n",
    "            app_name=app_name, \n",
    "            user_id=user_id, \n",
    "            session_id=session_name\n",
    "        )\n",
    "        messages = fresh_session.events\n",
    "        print(f\"DEBUG: Found {len(messages)} messages to save.\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error fetching session history: {e}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    log_data = {\n",
    "        \"session_meta\": {\n",
    "            \"app_name\": app_name,\n",
    "            \"session_id\": session_name,\n",
    "            \"saved_at\": timestamp,\n",
    "            \"total_messages\": len(messages)\n",
    "        },\n",
    "        \"conversation_history\": []\n",
    "    }\n",
    "\n",
    "    for msg in messages:\n",
    "        content_text = \"\"\n",
    "        if msg.content and msg.content.parts:\n",
    "            parts_text = []\n",
    "            for p in msg.content.parts:\n",
    "                if hasattr(p, 'text') and p.text:\n",
    "                    parts_text.append(p.text)\n",
    "            content_text = \" \".join(parts_text)\n",
    "        \n",
    "        log_data[\"conversation_history\"].append({\n",
    "            \"role\": msg.role,\n",
    "            \"content\": content_text\n",
    "        })\n",
    "\n",
    "\n",
    "    filename = f\"Research_Log_{session_name}_{timestamp}.json\"\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(log_data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\" Log Saved Successfully: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to save file: {e}\")\n",
    "\n",
    "print(\"save_log_manual created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a8d3165",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Event' object has no attribute 'role'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m save_full_session_log(\n\u001b[32m      2\u001b[39m     session_service=session_service,\n\u001b[32m      3\u001b[39m     app_name=APP_NAME,\n\u001b[32m      4\u001b[39m     user_id=USER_ID,\n\u001b[32m      5\u001b[39m     session_id=SESSION\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36msave_full_session_log\u001b[39m\u001b[34m(session_service, app_name, user_id, session_id)\u001b[39m\n\u001b[32m     41\u001b[39m             content_text = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(parts_text)\n\u001b[32m     43\u001b[39m         \u001b[38;5;66;03m# এন্ট্রি তৈরি\u001b[39;00m\n\u001b[32m     44\u001b[39m         entry = {\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mmsg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrole\u001b[49m,  \u001b[38;5;66;03m# user বা model\u001b[39;00m\n\u001b[32m     46\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: content_text\n\u001b[32m     47\u001b[39m         }\n\u001b[32m     48\u001b[39m         log_data[\u001b[33m\"\u001b[39m\u001b[33mconversation_history\u001b[39m\u001b[33m\"\u001b[39m].append(entry)\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# ৪. ফাইলে সেভ করা\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pydantic\\main.py:1026\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1025\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Event' object has no attribute 'role'"
     ]
    }
   ],
   "source": [
    "\n",
    "await save_full_session_log(\n",
    "    session_service=session_service,\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
